# -*- coding: utf-8 -*-
"""XGBoost_Demand_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HqbGEgz3EcKh4Exk6L7h2-QbufA7r39M

**Objective**

Train and evaluate a machine learning model (XGBoost Regressor) to support dynamic pricing decisions by predicting demand (Units Sold).
Validate the effectiveness of ML-based pricing using historical backtesting and revenue lift analysis, and compare it against static and rule-based pricing.
"""

import pandas as pd
import numpy as np

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/Copy of PriceOptima_FeatureEngineered_Final.csv",parse_dates=['Date'])

df = df.sort_values('Date').reset_index(drop=True)
df.shape

# Check missing values
df.isnull().sum().sum()

# Check duplicates
df.duplicated().sum()

y = df['Units Sold']

drop_cols = [
    'Units Sold',
    'Revenue',
    'daily_profit'
]

X = df.drop(columns=drop_cols)

X.shape, y.shape

# Columns not suitable for XGBoost
invalid_cols = ['Date', 'Seasonality']

X = X.drop(columns=invalid_cols)

"""Raw datetime and object-type categorical columns (Date, Seasonality) were excluded from model training.
Instead, engineered numeric representations (day, month, year, weekend flags, season encodings) were used, ensuring compatibility with XGBoost and preventing information leakage.
"""

split_date = df['Date'].quantile(0.80)

X_train = X[df['Date'] <= split_date]
X_test  = X[df['Date'] > split_date]

y_train = y[df['Date'] <= split_date]
y_test  = y[df['Date'] > split_date]

print("Train set:", X_train.shape)
print("Test set:", X_test.shape)

"""Time-based splitting ensures the model is trained on past data and evaluated on future data, preventing look-ahead bias."""

xgb_model = XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='reg:squarederror',
    random_state=42
)

xgb_model.fit(X_train, y_train)

y_pred = xgb_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("XGBoost RMSE:", rmse)
print("XGBoost MAE:", mae)

"""Pricing Simulation Logic

Higher predicted demand → price increase

Lower predicted demand → price decrease
"""

test_df = df[df['Date'] > split_date].copy()

test_df['predicted_demand'] = y_pred

demand_ratio = test_df['predicted_demand'] / test_df['avg_demand_7d']

test_df['ml_price'] = test_df['Price'] * (
    1 + 0.05 * (demand_ratio - 1)
)

# Safety cap
test_df['ml_price'] = test_df['ml_price'].clip(
    lower=0.8 * test_df['Price'],
    upper=1.2 * test_df['Price']
)

test_df[['Price','ml_price']].describe()

# How much did prices change on average?
test_df['price_change_pct_ml'] = (test_df['ml_price'] - test_df['Price']) / test_df['Price'] * 100
test_df['price_change_pct_ml'].describe()

test_df['static_revenue'] = test_df['Price'] * test_df['Units Sold']
test_df['ml_revenue'] = test_df['ml_price'] * test_df['Units Sold']

static_total = test_df['static_revenue'].sum()
ml_total = test_df['ml_revenue'].sum()

revenue_lift_pct = (ml_total - static_total) / static_total * 100

static_total, ml_total, revenue_lift_pct

revenue_comparison = pd.DataFrame({
    "Pricing Strategy": ["Static Pricing", "ML-Based Pricing"],
    "Total Revenue": [static_total, ml_total]
})

revenue_comparison

test_df['ml_action'] = np.where(
    test_df['ml_price'] > test_df['Price'], 'Increase',
    np.where(test_df['ml_price'] < test_df['Price'], 'Decrease', 'No Change')
)

test_df['ml_action'].value_counts(normalize=True) * 100

